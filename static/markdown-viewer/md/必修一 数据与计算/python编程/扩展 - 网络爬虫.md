# Python 爬虫入门教程

本教程将带你从零开始学习编写简单的 Python 爬虫，适合初学者。
**学习爬虫前请务必注意爬虫的法律风险，因为爬取网站获取数据进行商业活动导致判刑的例子屡见不鲜**。
## 目录
1. [爬虫基础概念](#基础概念)
2. [环境准备](#环境准备)
3. [第一个简单爬虫](#第一个简单爬虫)
4. [使用 BeautifulSoup 解析 HTML](#使用-beautifulsoup-解析-html)
5. [处理动态内容](#处理动态内容)
6. [遵守爬虫道德规范](#遵守爬虫道德规范)
7. [完整示例](#完整示例)

## 基础概念

**网络爬虫**是一种自动浏览网页并提取信息的程序。主要用途包括：
- 搜索引擎索引
- 数据采集和分析
- 价格监控
- 内容聚合

## 环境准备

首先安装必要的库（需在命令行实现，若是thonny等虚拟环境，可在"工具"-"打开系统shell"中输入命令）：

```bash
pip install requests beautifulsoup4 selenium lxml
```

## 第一个简单爬虫

```python
import requests

# 发送 HTTP 请求
url = "http://httpbin.org/get"
response = requests.get(url)

# 检查请求是否成功
if response.status_code == 200:
    print("请求成功!")
    print("响应内容:", response.text)
else:
    print(f"请求失败，状态码: {response.status_code}")
```

## 使用 BeautifulSoup 解析 HTML

```python
import requests
from bs4 import BeautifulSoup

def simple_crawler():
    # 目标网址（以豆瓣电影为例）
    url = "https://movie.douban.com/top250"
    
    # 设置请求头，模拟浏览器访问
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        # 发送请求
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # 如果状态码不是200，抛出异常
        
        # 解析 HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 查找电影信息
        movies = []
        movie_items = soup.find_all('div', class_='info')
        
        for item in movie_items[:5]:  # 只取前5部电影
            title = item.find('span', class_='title').text
            rating = item.find('span', class_='rating_num').text
            quote_elem = item.find('span', class_='inq')
            quote = quote_elem.text if quote_elem else "暂无引用"
            
            movies.append({
                'title': title,
                'rating': rating,
                'quote': quote
            })
        
        # 打印结果
        for i, movie in enumerate(movies, 1):
            print(f"{i}. 电影: {movie['title']}")
            print(f"   评分: {movie['rating']}")
            print(f"   引用: {movie['quote']}")
            print()
            
    except requests.RequestException as e:
        print(f"请求出错: {e}")

if __name__ == "__main__":
    simple_crawler()
```

## 处理动态内容

对于使用 JavaScript 动态加载内容的网站，可以使用 Selenium：

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

def dynamic_content_crawler():
    # 设置 Chrome 选项
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # 无头模式，不显示浏览器窗口
    
    # 启动浏览器
    driver = webdriver.Chrome(options=chrome_options)
    
    try:
        # 访问页面
        url = "https://example.com"  # 替换为目标网址
        driver.get(url)
        
        # 等待页面加载
        time.sleep(3)
        
        # 获取页面内容
        page_source = driver.page_source
        
        # 使用 BeautifulSoup 解析
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # 提取需要的信息...
        # 你的解析代码 here
        
    finally:
        # 关闭浏览器
        driver.quit()

# 注意：使用前需要下载对应浏览器的驱动
```

## 遵守爬虫道德规范

编写爬虫时请遵守以下原则：

1. **尊重 robots.txt**：检查网站的 robots.txt 文件
2. **设置合理的请求间隔**：避免对服务器造成压力
3. **识别自己**：在 User-Agent 中表明身份
4. **遵守网站条款**：不要爬取禁止爬取的内容
5. **不要过度请求**：控制爬取频率和数据量

```python
import time
import requests
from urllib.robotparser import RobotFileParser

def polite_crawler(url):
    # 检查 robots.txt
    rp = RobotFileParser()
    robots_url = '/'.join(url.split('/')[:3]) + '/robots.txt'
    rp.set_url(robots_url)
    rp.read()
    
    if not rp.can_fetch('*', url):
        print(f"根据 robots.txt，不允许爬取: {url}")
        return
    
    # 设置礼貌的请求头
    headers = {
        'User-Agent': 'MyCrawler/1.0 (educational purpose)',
        'From': 'your-email@example.com'  # 可选，提供联系方式
    }
    
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            # 处理响应...
            print("爬取成功")
            
        # 礼貌等待
        time.sleep(1)
        
    except requests.RequestException as e:
        print(f"请求失败: {e}")
```

## 完整示例

下面是一个完整的简单爬虫示例，爬取 quotes.toscrape.com 的名言：

```python
import requests
from bs4 import BeautifulSoup
import time
import csv

def quotes_crawler():
    base_url = "http://quotes.toscrape.com"
    headers = {
        'User-Agent': 'LearningCrawler/1.0'
    }
    
    quotes_data = []
    page = 1
    
    while True:
        print(f"正在爬取第 {page} 页...")
        
        if page == 1:
            url = base_url
        else:
            url = f"{base_url}/page/{page}"
        
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            quotes = soup.find_all('div', class_='quote')
            
            # 如果没有找到名言，说明已经到达最后一页
            if not quotes:
                print("已到达最后一页")
                break
            
            for quote in quotes:
                text = quote.find('span', class_='text').text
                author = quote.find('small', class_='author').text
                tags = [tag.text for tag in quote.find_all('a', class_='tag')]
                
                quotes_data.append({
                    'text': text,
                    'author': author,
                    'tags': ', '.join(tags)
                })
            
            # 检查是否有下一页
            next_button = soup.find('li', class_='next')
            if not next_button:
                break
                
            page += 1
            time.sleep(1)  # 礼貌等待
            
        except requests.RequestException as e:
            print(f"爬取第 {page} 页时出错: {e}")
            break
    
    # 保存数据到 CSV 文件
    if quotes_data:
        with open('quotes.csv', 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['text', 'author', 'tags']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for quote in quotes_data:
                writer.writerow(quote)
        
        print(f"成功爬取 {len(quotes_data)} 条名言，已保存到 quotes.csv")
    
    return quotes_data

if __name__ == "__main__":
    quotes = quotes_crawler()
    
    # 打印前5条结果
    for i, quote in enumerate(quotes[:5], 1):
        print(f"{i}. {quote['text']}")
        print(f"   — {quote['author']}")
        print(f"   标签: {quote['tags']}")
        print()
```

## 下一步学习建议

1. **学习正则表达式**：用于更复杂的文本匹配
2. **掌握 Scrapy 框架**：专业的 Python 爬虫框架
3. **了解反爬虫机制**：学习处理验证码、IP封锁等问题
4. **学习数据存储**：将爬取的数据保存到数据库
5. **学习异步爬虫**：提高爬取效率

记住：爬虫技术要合法使用，尊重网站规则和隐私政策！